{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Activity 5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdnfw7dGjyCx",
        "colab_type": "text"
      },
      "source": [
        "# 2019/12/03 CoE 202 Activity 5\n",
        "\n",
        "### **Name Classification**<br/>\n",
        "\n",
        "**Professor: Yong Hoon, Lee**</br>\n",
        "\n",
        "**TA : Seungjun moon, Beomgu Kang**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TA1cttLjyCy",
        "colab_type": "code",
        "outputId": "856dba59-00d4-42d9-ee2c-a571694af756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os.path\n",
        "import string\n",
        "\n",
        "model_save_path = 'tmp/model.ckpt'\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BezfImM-jyC0",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNsy2f3pjyC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate       = 0.005\n",
        "n_epoch             = 200\n",
        "n_hidden            = 128 # hidden layer features\n",
        "max_sequence_length = 19 # maximum number of characters is 19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9glTsxcjyC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_input     = len(all_letters)\n",
        "alphabet    = all_letters\n",
        "ethnicities = ['Chinese', 'Japanese', 'Vietnamese', 'Korean', 'Arabic','Czech','Dutch','English','French','German','Greek','Irish','Italian','Polish','Portuguese','Russian','Scottish','Spanish']\n",
        "n_classes   = len(ethnicities) # the number of classes\n",
        "\n",
        "name_strings, ethnicity_strings, str_list, names_list, ethnicity_list = [], [], [], [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPTLnButjyC5",
        "colab_type": "text"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bbj_wKljyC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi2E44kmjyC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5AYLEEtjyC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def name_one_hot(name, max_sequence_length):\n",
        "    result = []\n",
        "    for char in name:\n",
        "        v = np.zeros(n_input, dtype=np.int) # count space as a character\n",
        "        v[alphabet.index(char)] = 1\n",
        "        result.append(v)\n",
        "    while len(result) < max_sequence_length:\n",
        "        result.append(np.zeros(n_input, dtype=np.int))\n",
        "    result = np.array(result)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcihtCMOjyC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ethnicity_one_hot(ethnicity):\n",
        "    v = np.zeros(n_classes, dtype=np.int)\n",
        "    v[ethnicities.index(ethnicity)] = 1\n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB5QoncIjyDC",
        "colab_type": "text"
      },
      "source": [
        "## Data load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEzzdstLjyDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('names_revised.csv', 'r') as csv:\n",
        "    for line in csv:       \n",
        "        l = [s.strip() for s in line.split(',')] # lowercase L, not capital i , l['name', 'ehnicity']\n",
        "        if(l[1] in ethnicities):\n",
        "            name_strings.append(l[0])\n",
        "            ethnicity_strings.append(l[1])\n",
        "            if len(l[0]) > max_sequence_length:\n",
        "                l[0] = l[0][:max_sequence_length]\n",
        "            names_list.append(name_one_hot(l[0], max_sequence_length)) # one-hot vector of each characters of name\n",
        "            ethnicity_list.append(ethnicity_one_hot(l[1])) # one-hot vector of ethnicity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO7yNomcjyDF",
        "colab_type": "text"
      },
      "source": [
        "## Training - Test Seperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0U5s-pvjyDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng_state = np.random.get_state() # use the same random number generator state\n",
        "np.random.shuffle(names_list)     # when shuffling the two lists\n",
        "np.random.set_state(rng_state)    # they are effectively shuffled in parallel so that inputs still correspond to outputs after shuffling\n",
        "np.random.shuffle(ethnicity_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yz1LcxgjyDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = len(names_list) \n",
        "train_size = np.int(size*2/3) \n",
        "\n",
        "training_X = np.array(names_list[:train_size])\n",
        "training_y = np.array(ethnicity_list[:train_size])\n",
        "testing_X = np.array(names_list[train_size:])\n",
        "testing_y = np.array(ethnicity_list[train_size:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVWPe9fsjyDL",
        "colab_type": "text"
      },
      "source": [
        "## Build a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvnMMU5jjyDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, [None, max_sequence_length, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSlf-b6BjyDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_weights = weight_variable([n_hidden, n_classes])\n",
        "out_biases = bias_variable([n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "NpGotiV8jyDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic RNN\n",
        "# cells = tf.contrib.rnn.BasicRNNCell(num_units = 128)\n",
        "# LSTM\n",
        "# cells = tf.contrib.rnn.BasicLSTMCell(num_units = 128)\n",
        "# GRU\n",
        "# cells = tf.contrib.rnn.GRUCell(num_units = 128)\n",
        "# Modified DNN\n",
        "x = tf.reshape(X, [-1, max_sequence_length * n_input])\n",
        "\n",
        "w_init = tf.variance_scaling_initializer()\n",
        "b_init = tf.constant_initializer(0.)\n",
        "\n",
        "## 1st hidden layer\n",
        "w1 = tf.get_variable('weight1', [max_sequence_length * n_input, 256], initializer=w_init)      # weight for 1st hidden layer which have 256 units\n",
        "b1 = tf.get_variable('biases1', [256], initializer=b_init)                                     # bias for 1st hidden layer which have 256 units\n",
        "h1  = tf.matmul(x, w1) + b1                                                                    # matrix multiplication\n",
        "h1  = tf.nn.relu(h1)                                                                           # relu activation\n",
        "\n",
        "## 2nd hidden layer\n",
        "w2 = tf.get_variable('weight2', [256, 256], initializer=w_init)                                # weight for 2nd hidden layer which have 256 units\n",
        "b2 = tf.get_variable('biases2', [256], initializer=b_init)                                     # bias for 2nd hidden layer which have 256 units\n",
        "h2  = tf.matmul(h1, w2) + b2                                                                   # matrix multiplication\n",
        "h2  = tf.nn.relu(h2)                                                                           # relu activation\n",
        "\n",
        "## output layer\n",
        "w3 = tf.get_variable('weight3', [256, 18], initializer=w_init)                                 # weight for output layer which have 18 classes\n",
        "\n",
        "y_ = tf.matmul(h2, w3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU4l1ZYkjyDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_ = tf.matmul(outputs[:,-1,:], out_weights) + out_biases # predict y based on final rnn output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxb-kBQljyDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_, labels=y))\n",
        "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glBqL6gnjyDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation\n",
        "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRaFa1mvjyDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Softmax\n",
        "pred = tf.nn.softmax(y_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyloTl0jyDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKPwMADjjyDZ",
        "colab_type": "text"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wawhLF-tjyDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QkJ2K7nVjyDd",
        "colab_type": "code",
        "outputId": "fec176b5-86e3-41a7-eb2c-ee6347a27782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        }
      },
      "source": [
        "\n",
        "\n",
        "for _ in range(n_epoch+1):\n",
        "    sess.run(train_step, feed_dict={X: training_X, y: training_y})\n",
        "    if _%10 == 0:\n",
        "        train_accuracy = accuracy.eval(feed_dict={X:training_X, y:training_y})\n",
        "        print(\"step %d, training accuracy %g\"%(_, train_accuracy))\n",
        "        test_accuracy = accuracy.eval(feed_dict={X:testing_X, y:testing_y})\n",
        "        print(\"testing accuracy\", test_accuracy)\n",
        "saver.save(sess, model_save_path)\n",
        "print(\"Model saved in file: %s\" % model_save_path)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.471898\n",
            "testing accuracy 0.46196383\n",
            "step 10, training accuracy 0.479596\n",
            "testing accuracy 0.47018382\n",
            "step 20, training accuracy 0.654709\n",
            "testing accuracy 0.63682556\n",
            "step 30, training accuracy 0.715321\n",
            "testing accuracy 0.69406664\n",
            "step 40, training accuracy 0.760688\n",
            "testing accuracy 0.729039\n",
            "step 50, training accuracy 0.803812\n",
            "testing accuracy 0.75683755\n",
            "step 60, training accuracy 0.844768\n",
            "testing accuracy 0.78314155\n",
            "step 70, training accuracy 0.885501\n",
            "testing accuracy 0.7868779\n",
            "step 80, training accuracy 0.91719\n",
            "testing accuracy 0.7850844\n",
            "step 90, training accuracy 0.941704\n",
            "testing accuracy 0.78538334\n",
            "step 100, training accuracy 0.957474\n",
            "testing accuracy 0.7834405\n",
            "step 110, training accuracy 0.967339\n",
            "testing accuracy 0.7806008\n",
            "step 120, training accuracy 0.972646\n",
            "testing accuracy 0.7785084\n",
            "step 130, training accuracy 0.976158\n",
            "testing accuracy 0.77611715\n",
            "step 140, training accuracy 0.977803\n",
            "testing accuracy 0.7740248\n",
            "step 150, training accuracy 0.978625\n",
            "testing accuracy 0.7731281\n",
            "step 160, training accuracy 0.979073\n",
            "testing accuracy 0.76998955\n",
            "step 170, training accuracy 0.979073\n",
            "testing accuracy 0.77028847\n",
            "step 180, training accuracy 0.979073\n",
            "testing accuracy 0.770139\n",
            "step 190, training accuracy 0.979073\n",
            "testing accuracy 0.770139\n",
            "step 200, training accuracy 0.979073\n",
            "testing accuracy 0.76969063\n",
            "Model saved in file: tmp/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf-Ax6YxjyDf",
        "colab_type": "code",
        "outputId": "3e1936d2-6f56-4a6d-d578-9a6efb36d6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "i=0\n",
        "while i<5:\n",
        "    input_name = input('Enter a last name (max 19 letters):')\n",
        "   \n",
        "    while len(input_name) > max_sequence_length or len(input_name) == 0:\n",
        "        input_name = raw_input('Invalid input. Enter a last name (max 19 letters):')\n",
        "   \n",
        "    result=pred.eval(feed_dict={X: np.expand_dims(name_one_hot(input_name, 19), axis=0)})[0]\n",
        "    idx = np.argsort(result)[::-1]\n",
        "    print(\"\\n(%s): %.4f\" % (ethnicities[idx[0]], result[idx[0]]))\n",
        "    print(\"(%s): %.4f\" % (ethnicities[idx[1]], result[idx[1]]))\n",
        "    print(\"(%s): %.4f\" % (ethnicities[idx[2]], result[idx[2]]))\n",
        "    print(\"==========================================\")\n",
        "    i=i+1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a last name (max 19 letters):Kim\n",
            "\n",
            "(Korean): 0.5832\n",
            "(Vietnamese): 0.3729\n",
            "(English): 0.0190\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):Han\n",
            "\n",
            "(Vietnamese): 0.4924\n",
            "(Korean): 0.4850\n",
            "(Chinese): 0.0139\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):blanc\n",
            "\n",
            "(English): 0.9708\n",
            "(Czech): 0.0205\n",
            "(Spanish): 0.0075\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):sebastian\n",
            "\n",
            "(Russian): 0.9997\n",
            "(Greek): 0.0003\n",
            "(Italian): 0.0000\n",
            "==========================================\n",
            "Enter a last name (max 19 letters):andrew\n",
            "\n",
            "(English): 0.9592\n",
            "(Scottish): 0.0323\n",
            "(Dutch): 0.0082\n",
            "==========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeaWPKY5jyDg",
        "colab_type": "text"
      },
      "source": [
        "## 4. Report\n",
        "\n",
        "### a. Use GRU, LSTM and Simple RNN functions for training . Compare each of results.\n",
        "\n",
        "### b. Replace the RNN with DNN as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FIJnwWijyDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.reshape(X, [-1, max_sequence_length * n_input])\n",
        "\n",
        "w_init = tf.variance_scaling_initializer()\n",
        "b_init = tf.constant_initializer(0.)\n",
        "\n",
        "## 1st hidden layer\n",
        "w1 = tf.get_variable('weight1', [max_sequence_length * n_input, 256], initializer=w_init)      # weight for 1st hidden layer which have 256 units\n",
        "b1 = tf.get_variable('biases1', [256], initializer=b_init)                                     # bias for 1st hidden layer which have 256 units\n",
        "h1  = tf.matmul(x, w1) + b1                                                                    # matrix multiplication\n",
        "h1  = tf.nn.relu(h1)                                                                           # relu activation\n",
        "\n",
        "## 2nd hidden layer\n",
        "w2 = tf.get_variable('weight2', [256, 256], initializer=w_init)                                # weight for 2nd hidden layer which have 256 units\n",
        "b2 = tf.get_variable('biases2', [256], initializer=b_init)                                     # bias for 2nd hidden layer which have 256 units\n",
        "h2  = tf.matmul(h1, w2) + b2                                                                   # matrix multiplication\n",
        "h2  = tf.nn.relu(h2)                                                                           # relu activation\n",
        "\n",
        "## output layer\n",
        "w3 = tf.get_variable('weight3', [256, 18], initializer=w_init)                                 # weight for output layer which have 18 classes\n",
        "\n",
        "y_ = tf.matmul(h2, w3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeCg_DzxjyDj",
        "colab_type": "text"
      },
      "source": [
        "### Submission (Due: Dec. 10 Tue.)\n",
        "Submit your report by Tuesday, December 10 to **\"june1212@kaist.ac.kr\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6QB2DMJjyDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}