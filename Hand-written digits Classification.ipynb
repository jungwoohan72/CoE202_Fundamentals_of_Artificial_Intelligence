{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 4. MNIST Hand-written digit classification\n",
    "\n",
    "### **2019/11/21 CoE 202 Fall**<br/>\n",
    "\n",
    "**Professor: Yong Hoon, Lee**</br>\n",
    "\n",
    "**TA:Beomgu Kang, Seungjun moon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.training import moving_averages\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "### <a href=http://yann.lecun.com/exdb/mnist/>MNIST dataset</a>\n",
    "The MNIST has a training set of 55,000 examples, a validation set of 5,000 examples and a test set of 10,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('./data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train.images[:1000]\n",
    "train_labels = mnist.train.labels[:1000]\n",
    "train_images = train_images.reshape([-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = mnist.validation.images\n",
    "val_labels = mnist.validation.labels\n",
    "val_images = val_images.reshape([-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 1st hand-written digit and its one-hot label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0,:,:,0], cmap='Greys')\n",
    "print(\"\\nOne-hot labels for this image:\")\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a graph for VGGNet (Oxford, 2014)\n",
    "Very Deep Convolutional Networks for Large-scale image recognition\n",
    "https://arxiv.org/abs/1409.1556\n",
    "\n",
    "<img src=\"img/fig2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters\n",
    "- ***log_dir*** : Directory name to save models\n",
    "- ***n_epochs*** : Maximun training epoch\n",
    "- ***n_outputs*** : The number of classes for labels\n",
    "- ***init_lr*** : Learning rate for gradient descent\n",
    "- ***l2_lambda*** : regularization parameter\n",
    "- ***batch_size*** : The number of images to update paramerters once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'logs/'\n",
    "n_epochs = 20\n",
    "n_outputs = 10\n",
    "init_lr = 0.01\n",
    "batch_size = 100\n",
    "l2_lambda = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for learning rate, input images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_rate = tf.placeholder(tf.float32, shape=(), name='lrn_rate')\n",
    "images = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name='images')\n",
    "labels = tf.placeholder(tf.int32, shape=(None), name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vggnet(images, labels=None):\n",
    "    vggnet_conv = partial(tf.layers.conv2d, kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_lambda), padding=\"SAME\")\n",
    "    \n",
    "    ''' 1st conv. '''\n",
    "    x = vggnet_conv(images, filters=16, kernel_size=7, strides=[1,1])  # 7x7 filter, # of filters: 16, stride: 1\n",
    "    x = tf.layers.batch_normalization(x, name='bn1')                   # batch normalization\n",
    "    x = tf.nn.relu(x)                                                  # ReLU activation\n",
    "    \n",
    "    ''' 2nd conv.'''\n",
    "    x = vggnet_conv(x, filters=16, kernel_size=3, strides=[1,1])       # 3x3 filter, # of filters: 16, stride 1\n",
    "    x = tf.layers.batch_normalization(x, name='bn2')                   # batch normalization\n",
    "    x = tf.nn.relu(x)                                                  # ReLU activation\n",
    "    x = tf.layers.average_pooling2d(x, pool_size=[2,2], strides=[2,2]) # 2x2 average pooling, stride: 2\n",
    "    \n",
    "    ''' 3rd conv. '''\n",
    "    x = vggnet_conv(x, filters=32, kernel_size=3, strides=[1, 1])      # 3x3 filter, # of filters: 32, stride: 1\n",
    "    x = tf.layers.batch_normalization(x, name='bn3')                   # batch normalization\n",
    "    x = tf.nn.relu(x)                                                  # ReLU activation\n",
    "    \n",
    "    ''' 4th conv. '''\n",
    "    x = vggnet_conv(x, filters=32, kernel_size=3, strides=[1,1])       # 3x3 filter, # of filters: 32, stride 1\n",
    "    x = tf.layers.batch_normalization(x, name='bn4')                   # batch normalization\n",
    "    x = tf.nn.relu(x)                                                  # ReLU activation\n",
    "    x = tf.layers.average_pooling2d(x, pool_size=[2,2], strides=[2,2]) # 2x2 average pooling stride 2\n",
    "    \n",
    "    ''' 5th conv. '''\n",
    "    x = vggnet_conv(x, filters=64, kernel_size=3, strides=[1,1])       # 3x3 filter, # of filters: 64, stride: 1\n",
    "    x = tf.layers.batch_normalization(x, name='bn5')                   # batch normalization\n",
    "    x = tf.nn.relu(x)                                                  # ReLU activation\n",
    "    \n",
    "    ''' 6th conv. '''\n",
    "    x = vggnet_conv(x, filters=64, kernel_size=3, strides=[1,1])       # 3x3 filter, # of filters: 64, stride: 1\n",
    "    x = tf.layers.batch_normalization(x, name='bn6')                   # batch normalization\n",
    "    x = tf.nn.relu(x)                                                  # ReLU activation\n",
    "    \n",
    "    img_feat = tf.reduce_mean(x, [1, 2])                               # Global average pooling\n",
    "    return img_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "with tf.variable_scope('embed') as scope:\n",
    "#     feats = simple_network(images)\n",
    "    feats = vggnet(images)\n",
    "\n",
    "## Reshape\n",
    "feats = tf.reshape(feats, [batch_size, 64])\n",
    "    \n",
    "## Logits\n",
    "logits = tf.layers.dense(feats, n_outputs, kernel_initializer=tf.uniform_unit_scaling_initializer(factor=2.0), \n",
    "                                               kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_lambda))\n",
    "\n",
    "## Evaluation\n",
    "correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "## SOFTMAX\n",
    "preds = tf.nn.softmax(logits)\n",
    "\n",
    "## Cost function\n",
    "cent = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
    "cost_cls = tf.reduce_mean(cent, name='cent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "cost = tf.add_n([cost_cls] + reg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr  = tf.train.exponential_decay(init_lr, global_step, 1000, 0.8, staircase = True) # learning rate decay\n",
    "optimizer = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)                  # Momentum optimizer\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a session and initialize parameters\n",
    "Tensorflow operations must be executed in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE SESSION\n",
    "sess = tf.Session()\n",
    "\n",
    "## INITIALIZE SESSION\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates parameters with back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs+1):\n",
    "    for iteration in range(mnist.train.num_examples // batch_size):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        X_batch = X_batch.reshape([-1, 28, 28, 1])\n",
    "        (_, loss, loss_cls, prediction) = sess.run([train_op, cost, cost_cls, preds], \n",
    "                                                    feed_dict={images: X_batch, labels: y_batch})\n",
    "        duration = time.time() - start_time\n",
    "        sec_per_batch = float(duration)\n",
    "    \n",
    "    ## Training accuracy every one epoch\n",
    "    acc_train = accuracy.eval(session=sess, feed_dict={images: X_batch, labels: np.argmax(y_batch, axis=1)})\n",
    "    if epoch % 1 == 0:\n",
    "        print('  [*] TRAINING Iteration %d, Loss: %.4f, Acc: %.4f (duration: %.3fs)'\n",
    "                             % (epoch, loss_cls, acc_train, sec_per_batch))\n",
    "\n",
    "    ## Validation accuracy every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        acc_val = accuracy.eval(session=sess, feed_dict={images: val_images, labels: np.argmax(val_labels, axis=1)})\n",
    "        print('  [*] VALIDATION ACC: %.3f' % acc_val)\n",
    "\n",
    "print('Optimization done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ MNIST INPUTS\n",
    "test_images = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "test_images = test_images.reshape([-1, 28, 28, 1])\n",
    "\n",
    "## Plot the 1st test image and label\n",
    "plt.imshow(test_images[0,:,:,0], cmap='Greys')\n",
    "print(\"\\nOne-hot labels for this image:\")\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the prediction for the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sess.run(preds, feed_dict={images: test_images[0,:,:,0].reshape(1,28,28,1), labels: test_labels[0]})\n",
    "\n",
    "print(\"The prediction of the network is: %d\" % np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average the accuray for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = accuracy.eval(session=sess, feed_dict={images: test_images, labels: np.argmax(test_labels, axis=1)})\n",
    "print('Acc: %.3f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment<hr>\n",
    "### 1. A simple network for MNIST\n",
    "Design the following CNN and apply the CNN to the MNIST dataset.</br>\n",
    "\n",
    "<img src=\"img/fig3.png\">\n",
    "\n",
    "***Hint)*** \n",
    "- Elementwise-sum: tf.add() <br/>\n",
    "- Concatenate: tf.concat()\n",
    "\n",
    "\n",
    "### Submission (Due: Nov. 28 Thu.)\n",
    "Compare the results with that of VGGNET and submit your report by Thursday, November 28 to **\"almona@kaist.ac.kr\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
